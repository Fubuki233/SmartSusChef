{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmartSus Chef: The Universal Predictive Engine\n",
    "**Version:** 3.0 (Production Ready) | **Context:** Singapore & China | **Architecture:** Champion-Challenger\n",
    "\n",
    "## How to read this Notebook\n",
    "This notebook runs a robust, parallelized ML pipeline to predict food demand. It acts as a high-level **orchestrator**, delegating the complex implementation details to the `training_logic.py` module.\n",
    "\n",
    "### The End-to-End Workflow:\n",
    "1.  **Context Detection:** Automatically determines the restaurant's location (e.g., SG or CN) to load the correct holiday and weather data.\n",
    "2.  **Data Ingestion:** Fetches the raw sales history from a database or a fallback CSV file.\n",
    "3.  **Data Cleaning & Sanitation:** Aggregates data to have one record per dish-day and fills any gaps from missing sales days.\n",
    "4.  **Backtesting & Champion Selection:** For each dish, all three models (Prophet, CatBoost, XGBoost) are tuned using Optuna and evaluated via time-series cross-validation to find the \"champion\" with the lowest error.\n",
    "5.  **Parallel Execution:** The entire evaluation for each dish is run in parallel on a separate CPU core to significantly speed up the training process.\n",
    "6.  **Production Training:** After a champion model is chosen, all three models are retrained on 100% of the data using the best-found parameters and saved to disk.\n",
    "7.  **Forecasting:** A 14-day rolling forecast is generated using the saved production models, complete with SHAP-based explanations for a model's reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IMPORTS & SETUP ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import holidays\n",
    "import shap\n",
    "import openmeteo_requests\n",
    "from retry_requests import retry\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Import core pipeline logic from our module\n",
    "from training_logic import (\n",
    "    PipelineConfig,\n",
    "    CFG,\n",
    "    process_dish,\n",
    "    fetch_training_data,\n",
    "    add_local_context,\n",
    "    get_location_details,\n",
    "    safe_filename,\n",
    "    WEATHER_COLS,\n",
    ")\n",
    "\n",
    "print(\"Libraries loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Overview: From Raw Data to Trained Models\n",
    "\n",
    "The core of this project lies in the `training_logic.py` module, which is responsible for the heavy lifting. The notebook simply orchestrates the execution of this logic. The process for each dish is as follows:\n",
    "\n",
    "### 1. Data Preparation & Feature Engineering\n",
    "First, the pipeline prepares a clean, feature-rich dataset.\n",
    "- **Context & Ingestion:** It begins by detecting the restaurant's location (`add_local_context`) and loading the complete sales history (`fetch_training_data`).\n",
    "- **Cleaning & Enrichment:** The raw data is then cleaned to fill in missing dates (`sanitize_sparse_data`) and enriched with dozens of predictive features like lags, rolling averages, and trend indicators (`add_lag_features`). This provides rich input for the tree-based models.\n",
    "\n",
    "### 2. Backtesting & Champion Selection\n",
    "Next, each model type is rigorously tested to find the best one for the job.\n",
    "- **Time-Series Cross-Validation:** Instead of a simple train-test split, we use expanding-window cross-validation (`_generate_cv_folds`), which is more robust for time-series data.\n",
    "- **Hyperparameter Tuning:** For each model, `Optuna` is used to automatically find the best hyperparameters (e.g., learning rate, tree depth), ensuring peak performance.\n",
    "- **Champion Crowning:** The model with the lowest Mean Absolute Error (MAE) across the validation folds is crowned the \"champion\" for that specific dish.\n",
    "\n",
    "### 3. The `process_dish` Worker Function\n",
    "The entire data preparation and backtesting process is encapsulated within the `process_dish` function in `training_logic.py`. This function acts as a self-contained, parallel-ready worker that takes a dish name and returns the champion model, its performance metrics, and its ideal parameters. The cell below runs this function for all dishes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Multi-Day Prediction API (14-Day Rolling Forecast)\n",
    "The prediction API now returns a **list of forecasts** for the next 14 days.\n",
    "\n",
    "- **Tree models** use a **recursive forecasting loop**: each day's prediction is appended to the history to compute lags for the next day.\n",
    "- **Prophet** uses its native `make_future_dataframe()` for multi-step prediction.\n",
    "- **Average-only dishes** return a flat-line forecast at the saved average.\n",
    "- SHAP explanations use **name-based feature grouping** via `config.feature_groups` for robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODEL & GEOCODE CACHES ---\n",
    "_model_cache = {}\n",
    "_geocode_cache = {}\n",
    "_forecast_cache = {}\n",
    "\n",
    "def _load_cached(filepath):\n",
    "    if filepath not in _model_cache:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            _model_cache[filepath] = pickle.load(f)\n",
    "    return _model_cache[filepath]\n",
    "\n",
    "def clear_model_cache():\n",
    "    _model_cache.clear()\n",
    "    _geocode_cache.clear()\n",
    "    _forecast_cache.clear()\n",
    "\n",
    "\n",
    "def _get_location_cached(address):\n",
    "    \"\"\"Cached geocoding lookup to avoid redundant API calls.\"\"\"\n",
    "    if address not in _geocode_cache:\n",
    "        _geocode_cache[address] = get_location_details(address)\n",
    "    return _geocode_cache[address]\n",
    "\n",
    "\n",
    "def get_weather_forecast(latitude, longitude):\n",
    "    \"\"\"\n",
    "    Fetch 14-day weather forecast from the Open-Meteo Forecast API.\n",
    "    Returns a DataFrame with columns: date + WEATHER_COLS, or None if the API call fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        session = retry(retries=3, backoff_factor=0.5)\n",
    "        om = openmeteo_requests.Client(session=session)\n",
    "\n",
    "        url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "        params = {\n",
    "            \"latitude\": latitude,\n",
    "            \"longitude\": longitude,\n",
    "            \"daily\": [\"temperature_2m_max\", \"temperature_2m_min\",\n",
    "                      \"relative_humidity_2m_mean\", \"precipitation_sum\"],\n",
    "            \"forecast_days\": 16,\n",
    "            \"timezone\": \"auto\"\n",
    "        }\n",
    "\n",
    "        responses = om.weather_api(url, params=params)\n",
    "        response = responses[0]\n",
    "        daily = response.Daily()\n",
    "\n",
    "        dates = pd.date_range(\n",
    "            start=pd.to_datetime(daily.Time(), unit=\"s\", utc=True),\n",
    "            end=pd.to_datetime(daily.TimeEnd(), unit=\"s\", utc=True),\n",
    "            freq=pd.Timedelta(seconds=daily.Interval()),\n",
    "            inclusive=\"left\"\n",
    "        )\n",
    "\n",
    "        forecast_df = pd.DataFrame({\n",
    "            \"date\": dates,\n",
    "            \"temperature_2m_max\": daily.Variables(0).ValuesAsNumpy(),\n",
    "            \"temperature_2m_min\": daily.Variables(1).ValuesAsNumpy(),\n",
    "            \"relative_humidity_2m_mean\": daily.Variables(2).ValuesAsNumpy(),\n",
    "            \"precipitation_sum\": daily.Variables(3).ValuesAsNumpy(),\n",
    "        })\n",
    "        forecast_df['date'] = forecast_df['date'].dt.tz_localize(None).dt.normalize()\n",
    "\n",
    "        print(f\"Fetched {len(forecast_df)}-day weather forecast.\")\n",
    "        return forecast_df\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to fetch weather forecast: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def _get_forecast_cached(lat, lon):\n",
    "    \"\"\"Cached weather forecast lookup to avoid redundant API calls.\"\"\"\n",
    "    key = (round(lat, 4), round(lon, 4))\n",
    "    if key not in _forecast_cache:\n",
    "        _forecast_cache[key] = get_weather_forecast(lat, lon)\n",
    "    return _forecast_cache[key]\n",
    "\n",
    "\n",
    "def _compute_lag_features_from_history(sales_history, dt, config):\n",
    "    \"\"\"Compute all lag/rolling features for a single forecast date given a sales history array.\"\"\"\n",
    "    vals = sales_history\n",
    "    n = len(vals)\n",
    "\n",
    "    features = {}\n",
    "    for lag in [1, 7, 14, 21, 28]:\n",
    "        features[f'lag_{lag}'] = vals[-lag] if n >= lag else 0.0\n",
    "\n",
    "    if n >= 8:\n",
    "        window = vals[-8:-1]  # shifted by 1\n",
    "        features['rolling_mean_7'] = np.mean(window)\n",
    "        features['rolling_std_7'] = np.std(window, ddof=1) if len(window) > 1 else 0.0\n",
    "    else:\n",
    "        features['rolling_mean_7'] = np.mean(vals) if n > 0 else 0.0\n",
    "        features['rolling_std_7'] = 0.0\n",
    "\n",
    "    if n >= 15:\n",
    "        window = vals[-15:-1]\n",
    "        features['rolling_mean_14'] = np.mean(window)\n",
    "        features['rolling_std_14'] = np.std(window, ddof=1) if len(window) > 1 else 0.0\n",
    "    else:\n",
    "        features['rolling_mean_14'] = np.mean(vals) if n > 0 else 0.0\n",
    "        features['rolling_std_14'] = 0.0\n",
    "\n",
    "    rm7 = features['rolling_mean_7']\n",
    "    rm14 = features['rolling_mean_14']\n",
    "    features['trend_ratio'] = rm7 / rm14 if rm14 != 0 else 1.0\n",
    "\n",
    "    if n >= 2:\n",
    "        features['expanding_mean'] = np.mean(vals[:-1])\n",
    "    else:\n",
    "        features['expanding_mean'] = np.mean(vals) if n > 0 else 0.0\n",
    "\n",
    "    weekday_vals = [vals[-s] for s in [7, 14, 21, 28] if n >= s]\n",
    "    features['lag_same_weekday_avg'] = np.mean(weekday_vals) if weekday_vals else 0.0\n",
    "    features['lag_same_weekday_std'] = np.std(weekday_vals, ddof=1) if len(weekday_vals) > 1 else 0.0\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def _predict_tree_multiday(model_obj, forecast_weather_df, start_date,\n",
    "                           recent_sales_df, config, country_code, dish_mae):\n",
    "    \"\"\"\n",
    "    Recursive 14-day rolling forecast for tree-based models using real weather forecast.\n",
    "    Returns list of {date, qty, lower, upper, explanation} dicts.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    sales_history = recent_sales_df['sales'].values.tolist()\n",
    "\n",
    "    # Build feature name -> group mapping from config\n",
    "    feat_to_group = {}\n",
    "    for group_name, feat_list in config.feature_groups.items():\n",
    "        for feat in feat_list:\n",
    "            feat_to_group[feat] = group_name\n",
    "\n",
    "    local_hols = holidays.country_holidays(country_code, years=config.holiday_years)\n",
    "\n",
    "    for day_offset in range(config.forecast_horizon):\n",
    "        dt = start_date + pd.Timedelta(days=day_offset)\n",
    "        is_hol = 1 if dt in local_hols else 0\n",
    "\n",
    "        # Get weather from real forecast data for this date\n",
    "        weather_row = forecast_weather_df[\n",
    "            forecast_weather_df['date'].dt.normalize() == dt.normalize()\n",
    "        ]\n",
    "        if len(weather_row) > 0:\n",
    "            weather_vals = {col: float(weather_row[col].iloc[0]) for col in WEATHER_COLS}\n",
    "        else:\n",
    "            # Fallback: use forecast average for dates beyond range\n",
    "            weather_vals = {col: float(forecast_weather_df[col].mean()) for col in WEATHER_COLS}\n",
    "\n",
    "        lag_feats = _compute_lag_features_from_history(sales_history, dt, config)\n",
    "\n",
    "        row = {\n",
    "            'day_of_week': dt.dayofweek,\n",
    "            'month': dt.month,\n",
    "            'is_public_holiday': is_hol,\n",
    "        }\n",
    "        row.update(weather_vals)\n",
    "        row.update(lag_feats)\n",
    "\n",
    "        future_df = pd.DataFrame([row])[config.tree_features]\n",
    "        pred = float(model_obj.predict(future_df)[0])\n",
    "        qty = int(max(0, pred))\n",
    "        pred_lower = int(max(0, pred - dish_mae))\n",
    "        pred_upper = int(pred + dish_mae)\n",
    "\n",
    "        # SHAP explanation with name-based grouping\n",
    "        try:\n",
    "            ex = shap.TreeExplainer(model_obj)\n",
    "            sv = ex.shap_values(future_df)[0]\n",
    "            base_val = float(ex.expected_value)\n",
    "            group_shap = {}\n",
    "            for i, feat_name in enumerate(config.tree_features):\n",
    "                group = feat_to_group.get(feat_name, \"Other\")\n",
    "                group_shap[group] = group_shap.get(group, 0.0) + float(sv[i])\n",
    "\n",
    "            expl = {\n",
    "                \"Trend\": round(base_val + group_shap.get(\"Lags/Trend\", 0.0), 1),\n",
    "                \"Seasonality\": round(group_shap.get(\"Seasonality\", 0.0), 1),\n",
    "                \"Holiday\": round(group_shap.get(\"Holiday\", 0.0), 1),\n",
    "                \"Weather\": round(group_shap.get(\"Weather\", 0.0), 1),\n",
    "            }\n",
    "        except Exception:\n",
    "            expl = {\"Trend\": round(pred, 1), \"Seasonality\": 0.0,\n",
    "                    \"Holiday\": 0.0, \"Weather\": 0.0}\n",
    "\n",
    "        results.append({\n",
    "            \"date\": dt.strftime('%Y-%m-%d'),\n",
    "            \"qty\": qty,\n",
    "            \"lower\": pred_lower,\n",
    "            \"upper\": pred_upper,\n",
    "            \"explanation\": expl\n",
    "        })\n",
    "\n",
    "        # Append prediction to history for next iteration\n",
    "        sales_history.append(max(0, pred))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_prediction(dish, date_str, address, model='auto', config=CFG):\n",
    "    \"\"\"\n",
    "    Multi-day prediction API using real weather forecast data.\n",
    "    Returns a list of dicts (one per forecast day, up to config.forecast_horizon days).\n",
    "    For average-only dishes, returns flat-line forecast.\n",
    "    \"\"\"\n",
    "    dt = pd.to_datetime(date_str)\n",
    "\n",
    "    # Resolve location from address (cached)\n",
    "    lat, lon, country = _get_location_cached(address)\n",
    "    if lat is None:\n",
    "        country, lat, lon = 'CN', 31.23, 121.47\n",
    "    if not country or country not in holidays.list_supported_countries():\n",
    "        country = 'CN'\n",
    "\n",
    "    safe_name = safe_filename(dish)\n",
    "    dish_mae = 0.0\n",
    "\n",
    "    # Registry lookup\n",
    "    try:\n",
    "        registry = _load_cached(f'{config.model_dir}/champion_registry.pkl')\n",
    "        dish_info = registry[dish]\n",
    "        if model == 'auto':\n",
    "            model = dish_info['model']\n",
    "        dish_mae = dish_info['all_mae'].get(model, 0.0) if dish_info['all_mae'] else 0.0\n",
    "    except Exception:\n",
    "        if model == 'auto':\n",
    "            model = 'prophet'\n",
    "\n",
    "    # Average-only dishes\n",
    "    if model == 'average':\n",
    "        try:\n",
    "            avg_sales = _load_cached(f'{config.model_dir}/average_{safe_name}.pkl')\n",
    "        except Exception:\n",
    "            avg_sales = 0\n",
    "        results = []\n",
    "        for day_offset in range(config.forecast_horizon):\n",
    "            d = dt + pd.Timedelta(days=day_offset)\n",
    "            results.append({\n",
    "                \"Dish\": dish, \"Date\": d.strftime('%Y-%m-%d'),\n",
    "                \"Model Used\": \"AVERAGE\",\n",
    "                \"Prediction\": avg_sales,\n",
    "                \"Prediction_Lower\": avg_sales,\n",
    "                \"Prediction_Upper\": avg_sales,\n",
    "                \"Explanation\": {\"Trend\": float(avg_sales), \"Seasonality\": 0.0,\n",
    "                                \"Holiday\": 0.0, \"Weather\": 0.0}\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    # Get real weather forecast (cached) — no fallback to simulated data\n",
    "    forecast_weather = _get_forecast_cached(lat, lon)\n",
    "    if forecast_weather is None:\n",
    "        return [{\"Error\": \"Cannot generate forecast without valid weather forecast data\"}]\n",
    "\n",
    "    try:\n",
    "        if model == 'prophet':\n",
    "            mp = _load_cached(f'{config.model_dir}/prophet_{safe_name}.pkl')\n",
    "            future_dates = mp.make_future_dataframe(periods=config.forecast_horizon)\n",
    "            future_dates = future_dates.tail(config.forecast_horizon).copy()\n",
    "\n",
    "            # Merge real forecast weather data into Prophet's future dataframe\n",
    "            future_dates = future_dates.merge(\n",
    "                forecast_weather.rename(columns={'date': 'ds'}),\n",
    "                on='ds', how='left'\n",
    "            )\n",
    "            for col in WEATHER_COLS:\n",
    "                future_dates[col] = future_dates[col].ffill().bfill().fillna(0)\n",
    "\n",
    "            forecast = mp.predict(future_dates)\n",
    "\n",
    "            results = []\n",
    "            for _, row in forecast.iterrows():\n",
    "                yhat = row['yhat']\n",
    "                qty = int(max(0, yhat))\n",
    "                trend = row['trend']\n",
    "                holiday_val = row['holidays'] if 'holidays' in row else 0.0\n",
    "                weather = row['extra_regressors_additive'] if 'extra_regressors_additive' in row else 0.0\n",
    "                seasonality = yhat - trend - holiday_val - weather\n",
    "\n",
    "                results.append({\n",
    "                    \"Dish\": dish,\n",
    "                    \"Date\": row['ds'].strftime('%Y-%m-%d'),\n",
    "                    \"Model Used\": \"PROPHET\",\n",
    "                    \"Prediction\": qty,\n",
    "                    \"Prediction_Lower\": int(max(0, yhat - dish_mae)),\n",
    "                    \"Prediction_Upper\": int(yhat + dish_mae),\n",
    "                    \"Explanation\": {\n",
    "                        \"Trend\": round(trend, 1),\n",
    "                        \"Seasonality\": round(seasonality, 1),\n",
    "                        \"Holiday\": round(holiday_val, 1),\n",
    "                        \"Weather\": round(weather, 1)\n",
    "                    }\n",
    "                })\n",
    "            return results\n",
    "\n",
    "        elif model in ('catboost', 'xgboost'):\n",
    "            tree_model = _load_cached(f'{config.model_dir}/{model}_{safe_name}.pkl')\n",
    "            recent = _load_cached(f'{config.model_dir}/recent_sales_{safe_name}.pkl')\n",
    "\n",
    "            multiday = _predict_tree_multiday(\n",
    "                tree_model, forecast_weather, dt, recent, config, country, dish_mae)\n",
    "\n",
    "            results = []\n",
    "            for entry in multiday:\n",
    "                results.append({\n",
    "                    \"Dish\": dish,\n",
    "                    \"Date\": entry['date'],\n",
    "                    \"Model Used\": model.upper(),\n",
    "                    \"Prediction\": entry['qty'],\n",
    "                    \"Prediction_Lower\": entry['lower'],\n",
    "                    \"Prediction_Upper\": entry['upper'],\n",
    "                    \"Explanation\": entry['explanation']\n",
    "                })\n",
    "            return results\n",
    "\n",
    "    except Exception as e:\n",
    "        return [{\"Error\": f\"Model error for {dish}: {str(e)}\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Parallel Execution & Results\n",
    "Run all dishes in parallel using `ProcessPoolExecutor`. Each dish is independently processed across CPU cores.\n",
    "After completion, aggregate results into a leaderboard and save the champion registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PARALLEL EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load and prepare the master DataFrame\n",
    "    # This function also cleans the data by normalizing dates and aggregating sales.\n",
    "    raw_df = fetch_training_data()\n",
    "\n",
    "    # 2. Define global context for this run — provide an address or postal code\n",
    "    address_input = \"Shanghai, China\"\n",
    "    CFG = PipelineConfig()\n",
    "    enriched_df, country, lat, lon = add_local_context(raw_df, address_input)\n",
    "\n",
    "    # 4. Run the full training pipeline in parallel for each dish\n",
    "    unique_dishes = enriched_df['dish'].unique()\n",
    "    results = []\n",
    "\n",
    "    print(f\"\\n{'='*95}\")\n",
    "    print(f\"STARTING PARALLEL TRAINING FOR {len(unique_dishes)} DISHES \"\n",
    "          f\"({CFG.max_workers} workers, {CFG.n_optuna_trials} Optuna trials each)\")\n",
    "    print(f\"{'='*95}\")\n",
    "\n",
    "    # Use ProcessPoolExecutor to run the `process_dish` function for each dish on a separate core\n",
    "    with ProcessPoolExecutor(max_workers=CFG.max_workers) as executor:\n",
    "        # Create a dictionary mapping future objects to dish names for easy lookup\n",
    "        futures = {\n",
    "            executor.submit(process_dish, dish, enriched_df, country, CFG): dish\n",
    "            for dish in unique_dishes\n",
    "        }\n",
    "        # Process results as they are completed\n",
    "        for future in as_completed(futures):\n",
    "            dish_name = futures[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "                # Print real-time progress for each dish\n",
    "                if result['champion'] == 'average':\n",
    "                    print(f\"  {dish_name:<35} | AVG (short data) -> avg_sales={result['avg_sales']}\")\n",
    "                else:\n",
    "                    mae = result['mae']\n",
    "                    print(f\"  {dish_name:<35} | P={mae['prophet']:<7} C={mae['catboost']:<7} \"\n",
    "                          f\"X={mae['xgboost']:<7} -> {result['champion'].upper()}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  {dish_name:<35} | ERROR: {e}\")\n",
    "\n",
    "    # 5. Aggregate and display the final results from all parallel runs\n",
    "    champion_map = {}\n",
    "    all_predictions = {}\n",
    "    results_rows = []\n",
    "\n",
    "    for r in results:\n",
    "        dish = r['dish']\n",
    "        champion_map[dish] = {\n",
    "            'model': r['champion'],\n",
    "            'mae': r.get('champion_mae', 0.0),\n",
    "            'all_mae': r['mae']\n",
    "        }\n",
    "        if r['backtest_preds'] is not None:\n",
    "            all_predictions[dish] = r['backtest_preds']\n",
    "\n",
    "        if r['champion'] == 'average':\n",
    "            results_rows.append({\n",
    "                'Dish': dish, 'Prophet MAE': '-', 'CatBoost MAE': '-',\n",
    "                'XGBoost MAE': '-', 'Winner': 'AVERAGE'\n",
    "            })\n",
    "        else:\n",
    "            results_rows.append({\n",
    "                'Dish': dish,\n",
    "                'Prophet MAE': r['mae']['prophet'],\n",
    "                'CatBoost MAE': r['mae']['catboost'],\n",
    "                'XGBoost MAE': r['mae']['xgboost'],\n",
    "                'Winner': r['champion'].upper()\n",
    "            })\n",
    "\n",
    "    with open(f'{CFG.model_dir}/champion_registry.pkl', 'wb') as f:\n",
    "        pickle.dump(champion_map, f)\n",
    "\n",
    "    clear_model_cache()\n",
    "\n",
    "    results_table = pd.DataFrame(results_rows)\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"MODEL LEADERBOARD (Lower MAE is Better)\")\n",
    "    print(f\"{'='*50}\")\n",
    "    display(results_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- VISUALIZATION A: MAE Comparison Bar Chart ---\n",
    "# Filter to ML-trained dishes only (exclude average-only)\n",
    "ml_rows = results_table[results_table['Winner'] != 'AVERAGE'].copy()\n",
    "\n",
    "if len(ml_rows) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "    dishes = ml_rows['Dish']\n",
    "    x = np.arange(len(dishes))\n",
    "    width = 0.25\n",
    "\n",
    "    bars_p = ax.bar(x - width, ml_rows['Prophet MAE'].astype(float), width,\n",
    "                    label='Prophet', color='#4C72B0')\n",
    "    bars_c = ax.bar(x, ml_rows['CatBoost MAE'].astype(float), width,\n",
    "                    label='CatBoost', color='#DD8452')\n",
    "    bars_x = ax.bar(x + width, ml_rows['XGBoost MAE'].astype(float), width,\n",
    "                    label='XGBoost', color='#55A868')\n",
    "\n",
    "    for i, (_, row) in enumerate(ml_rows.iterrows()):\n",
    "        p_mae = float(row['Prophet MAE'])\n",
    "        c_mae = float(row['CatBoost MAE'])\n",
    "        x_mae = float(row['XGBoost MAE'])\n",
    "        winner_mae = min(p_mae, c_mae, x_mae)\n",
    "        if row['Winner'] == 'PROPHET':\n",
    "            offset = -width\n",
    "        elif row['Winner'] == 'CATBOOST':\n",
    "            offset = 0\n",
    "        else:\n",
    "            offset = width\n",
    "        ax.plot(x[i] + offset, winner_mae, marker='*', color='gold', markersize=14, zorder=5)\n",
    "\n",
    "    ax.set_xlabel('Dish')\n",
    "    ax.set_ylabel('MAE (plates)')\n",
    "    ax.set_title('Model MAE Comparison by Dish (lower is better)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(dishes, rotation=45, ha='right', fontsize=8)\n",
    "    ax.legend()\n",
    "    ax.yaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show average-only dishes if any\n",
    "avg_rows = results_table[results_table['Winner'] == 'AVERAGE']\n",
    "if len(avg_rows) > 0:\n",
    "    print(f\"\\nDishes using simple average (< {CFG.min_ml_days} days of data):\")\n",
    "    for _, row in avg_rows.iterrows():\n",
    "        print(f\"  - {row['Dish']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- VISUALIZATION B: Actual vs Predicted (Last Fold, Winning Model) ---\n",
    "# Only plot dishes that went through ML (have backtest predictions)\n",
    "ml_dishes = [r for r in results if r['champion'] != 'average' and r['backtest_preds'] is not None]\n",
    "\n",
    "if len(ml_dishes) > 0:\n",
    "    n_dishes = len(ml_dishes)\n",
    "    ncols = 4\n",
    "    nrows = int(np.ceil(n_dishes / ncols))\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(18, 4 * nrows), squeeze=False)\n",
    "\n",
    "    for idx, r in enumerate(ml_dishes):\n",
    "        ax = axes[idx // ncols][idx % ncols]\n",
    "        dish = r['dish']\n",
    "        winner = r['champion']\n",
    "\n",
    "        preds = r['backtest_preds'].get(winner)\n",
    "        if preds is not None:\n",
    "            dates = pd.to_datetime(preds['dates'])\n",
    "            ax.plot(dates, preds['actual'], label='Actual', color='#333333', linewidth=1.5)\n",
    "            ax.plot(dates, preds['predicted'], label='Predicted', color='#E24A33',\n",
    "                    linewidth=1.5, linestyle='--')\n",
    "            ax.fill_between(dates, preds['actual'], preds['predicted'],\n",
    "                            alpha=0.15, color='#E24A33')\n",
    "\n",
    "        ax.set_title(f\"{dish}\\n({winner.upper()})\", fontsize=8, fontweight='bold')\n",
    "        ax.tick_params(axis='x', rotation=30, labelsize=6)\n",
    "        ax.tick_params(axis='y', labelsize=7)\n",
    "        if idx == 0:\n",
    "            ax.legend(fontsize=7)\n",
    "\n",
    "    for idx in range(n_dishes, nrows * ncols):\n",
    "        axes[idx // ncols][idx % ncols].set_visible(False)\n",
    "\n",
    "    fig.suptitle('Actual vs Predicted Sales (Last CV Fold, Winning Model)', fontsize=13, y=1.01)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No ML-trained dishes to plot backtests for.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- VISUALIZATION C: Multi-Day Forecast (14 days) ---\n",
    "forecast_date = '2026-05-20'\n",
    "\n",
    "all_forecasts = {}\n",
    "day1_summary = []\n",
    "\n",
    "for dish_name in enriched_df['dish'].unique():\n",
    "    preds = get_prediction(\n",
    "        dish=dish_name, date_str=forecast_date,\n",
    "        address=address_input\n",
    "    )\n",
    "    if preds and 'Error' not in preds[0]:\n",
    "        all_forecasts[dish_name] = preds\n",
    "        p0 = preds[0]\n",
    "        day1_summary.append({\n",
    "            'Dish': p0['Dish'],\n",
    "            'Day 1 Qty': p0['Prediction'],\n",
    "            'Lower': p0['Prediction_Lower'],\n",
    "            'Upper': p0['Prediction_Upper'],\n",
    "            'Model': p0['Model Used'],\n",
    "            'Trend': p0['Explanation']['Trend'],\n",
    "            'Seasonality': p0['Explanation']['Seasonality'],\n",
    "            'Holiday': p0['Explanation']['Holiday'],\n",
    "            'Weather': p0['Explanation']['Weather']\n",
    "        })\n",
    "\n",
    "# Day-1 summary table\n",
    "day1_df = pd.DataFrame(day1_summary)\n",
    "print(f\"Forecast starting: {forecast_date} | Location: {address_input} | Horizon: {CFG.forecast_horizon} days\")\n",
    "print(f\"{'='*90}\")\n",
    "display(day1_df)\n",
    "\n",
    "# Line chart: 14-day forecast per dish with confidence bands\n",
    "n_dishes = len(all_forecasts)\n",
    "if n_dishes > 0:\n",
    "    ncols = 4\n",
    "    nrows = int(np.ceil(n_dishes / ncols))\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(18, 4 * nrows), squeeze=False)\n",
    "\n",
    "    colors_map = {'PROPHET': '#4C72B0', 'CATBOOST': '#DD8452', 'XGBOOST': '#55A868', 'AVERAGE': '#999999'}\n",
    "\n",
    "    for idx, (dish_name, preds) in enumerate(all_forecasts.items()):\n",
    "        ax = axes[idx // ncols][idx % ncols]\n",
    "        dates = [pd.to_datetime(p['Date']) for p in preds]\n",
    "        qtys = [p['Prediction'] for p in preds]\n",
    "        lowers = [p['Prediction_Lower'] for p in preds]\n",
    "        uppers = [p['Prediction_Upper'] for p in preds]\n",
    "        model_used = preds[0]['Model Used']\n",
    "        color = colors_map.get(model_used, '#333333')\n",
    "\n",
    "        ax.plot(dates, qtys, marker='o', markersize=3, color=color, linewidth=1.5,\n",
    "                label=model_used)\n",
    "        ax.fill_between(dates, lowers, uppers, alpha=0.2, color=color)\n",
    "\n",
    "        ax.set_title(f\"{dish_name}\\n({model_used})\", fontsize=8, fontweight='bold')\n",
    "        ax.tick_params(axis='x', rotation=30, labelsize=6)\n",
    "        ax.tick_params(axis='y', labelsize=7)\n",
    "        ax.legend(fontsize=7)\n",
    "\n",
    "    for idx in range(n_dishes, nrows * ncols):\n",
    "        axes[idx // ncols][idx % ncols].set_visible(False)\n",
    "\n",
    "    fig.suptitle(f'{CFG.forecast_horizon}-Day Rolling Forecast per Dish (with Confidence Bands)',\n",
    "                 fontsize=13, y=1.01)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda: dev)",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
